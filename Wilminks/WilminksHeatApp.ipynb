{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit, OptimizeWarning\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'FarmName_Pseudo': 'str',\n",
    "    'SE_Number': 'str',\n",
    "    'AnimalNumber': 'Int64',          \n",
    "    'StartDate': 'str',\n",
    "    'StartTime': 'str',\n",
    "    'DateTime': 'str',\n",
    "    'LactationNumber': 'Int64',       \n",
    "    'DaysInMilk': 'Int64', \n",
    "    'YearSeason': 'str',           \n",
    "    'TotalYield': 'float',\n",
    "    'DateTime': 'str',\n",
    "    'BreedName': 'str',\n",
    "    'Age': 'Int64',\n",
    "    'Mother': 'str',\n",
    "    'Father': 'str',\n",
    "    'CullDecisionDate': 'str',\n",
    "    'Temperature2': 'float',\n",
    "    'RelativeHumidity': 'float',      \n",
    "    'THI_adj2': 'float',\n",
    "    'HW': 'Int64',                    \n",
    "    'cum_HW': 'Int64',                \n",
    "    'Temp15Threshold': 'Int64'        \n",
    "}\n",
    "\n",
    "# Load the CSV with specified dtypes\n",
    "data = pd.read_csv('../Data/MergedData/CleanedYieldData.csv', dtype=dtype_dict)\n",
    "# data = pd.read_csv('../Data/MergedData/YieldData.csv', dtype=dtype_dict)\n",
    "# data = data[data[\"DaysInMilk\"].notna()]\n",
    "\n",
    "# Convert date and time columns back to datetime and time objects\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'], errors='coerce')\n",
    "data['StartTime'] = pd.to_datetime(data['StartTime'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "data['CullDecisionDate'] = pd.to_datetime(data['CullDecisionDate'], errors='coerce')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'], errors='coerce')\n",
    "\n",
    "print(f\"No. milking events in filtered dataset used for Wilmink's: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the DailyYield for each cow each day\n",
    "data['DailyYield'] = data.groupby(['SE_Number', 'StartDate'])['TotalYield'].transform('sum')\n",
    "\n",
    "# Sort the data by AnimalNumber and StartDate\n",
    "data.sort_values(['AnimalNumber', 'StartDate'], inplace=True)\n",
    "\n",
    "# Calculate the previous day's total yield for each cow\n",
    "data['PreviousDailyYield'] = data.groupby('AnimalNumber')['DailyYield'].shift(1)\n",
    "\n",
    "# Calculate the daily yield change for each cow\n",
    "data['DailyYieldChange'] = data['DailyYield'] - data['PreviousDailyYield']\n",
    "\n",
    "# Group and aggregate data ===========================================>>> OBS change Temperature to Temperature2 and THI_adj to THI_adj2 when running filtered data\n",
    "data = data.groupby(['SE_Number', 'FarmName_Pseudo', 'StartDate']).agg({\n",
    "    'DailyYield': 'first',\n",
    "    'PreviousDailyYield': 'first',\n",
    "    'DailyYieldChange': 'first',\n",
    "    'HW': 'max',\n",
    "    'Temperature2': 'mean',\n",
    "    'THI_adj2': 'mean',\n",
    "    'DaysInMilk': 'first',\n",
    "    'YearSeason': 'first',\n",
    "    'cum_HW': 'max',\n",
    "    'Temp15Threshold': 'max',\n",
    "    'Age': 'first',\n",
    "    'BreedName': 'first',\n",
    "    'LactationNumber': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Renaming and formatting ==============================================>>> OBS change Temperature to Temperature2 and THI_adj to THI_adj2 when running filtered data\n",
    "data.rename(columns={\n",
    "    'Temperature2': 'MeanTemperature',\n",
    "    'THI_adj2': 'MeanTHI_adj',\n",
    "    'StartDate': 'Date'\n",
    "}, inplace=True)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if DailyYield is centered around approx the same for each farm \n",
    "print(\"Mean of DailyYield:\", data.groupby('FarmName_Pseudo')['DailyYield'].mean())\n",
    "print(\"Standard Deviation of DailyYield:\", data.groupby('FarmName_Pseudo')['DailyYield'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wilmink Lactation Curve\n",
    "$$\n",
    "Y(t) = a + bt + c \\exp(-dt)\n",
    "$$\n",
    "- \\(Y(t)\\): Milk yield at time \\(t\\) post-calving, so t = DaysInMilk\n",
    "- \\(a\\): Intercept, representing baseline milk yield\n",
    "- \\(b\\): Linear increase rate of milk yield over time\n",
    "- \\(c\\): Initial exponential increase in milk yield\n",
    "- \\(d\\): Rate at which the exponential increase declines over time\n",
    "\n",
    "The Wilmink model captures the lactation curve by considering both linear and exponential components, providing a flexible representation of milk production dynamics over the lactation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    dim = np.array(dim, dtype=float)\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to detect and remove outliers\n",
    "def remove_outliers(group, threshold=3.5):\n",
    "    mean = np.mean(group['DailyYield'])\n",
    "    std_dev = np.std(group['DailyYield'])\n",
    "    return group[(group['DailyYield'] > mean - threshold * std_dev) & (group['DailyYield'] < mean + threshold * std_dev)]\n",
    "\n",
    "# Function to smooth the data using a rolling average\n",
    "def smooth_data(group, window=5):\n",
    "    group = group.copy()\n",
    "    group['DailyYield'] = group['DailyYield'].rolling(window, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "# Function to fit the Wilmink Lactation Curve to the dataset\n",
    "def fit_wilmink_lactation_curve(dataset):\n",
    "    # Initialize the 'ExpectedYield' column to NaN\n",
    "    dataset['ExpectedYield'] = np.nan\n",
    "    params_dict = {}\n",
    "    \n",
    "    valid_indices = []\n",
    "\n",
    "    # Group the dataset by 'SE_Number' and 'LactationNumber' and fit the curve for each segment\n",
    "    for (animal_number, lactation_number), group in tqdm(dataset.groupby(['SE_Number', 'LactationNumber']), unit=\" Segments\"):\n",
    "        # Prepare the data for fitting\n",
    "        group = remove_outliers(group, threshold=3.5)  # Remove outliers with threshold 4\n",
    "        group = smooth_data(group)  # Smooth the data\n",
    "        x_data = group['DaysInMilk'].values\n",
    "        y_data = group['DailyYield'].values\n",
    "        \n",
    "        # Ensure there are no NaN or infinite values in the data\n",
    "        if not np.isfinite(x_data).all() or not np.isfinite(y_data).all():\n",
    "            print(f\"Non-finite values found for cow {animal_number}, lactation {lactation_number}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure there are enough data points to fit the curve\n",
    "        if len(x_data) < 150 or len(y_data) < 150:\n",
    "            print(f\"Insufficient data points for cow {animal_number}, lactation {lactation_number}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        valid_indices.extend(group.index)\n",
    "        \n",
    "        # Fit the model\n",
    "        try:\n",
    "            # Initial parameter guesses\n",
    "            initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "            # Bounds on the parameters to prevent overflow\n",
    "            bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('error', category=OptimizeWarning)\n",
    "                try:\n",
    "                    popt, pcov = curve_fit(\n",
    "                        wilmink_lactation_curve, x_data, y_data,\n",
    "                        p0=initial_guesses, bounds=bounds, maxfev=30000\n",
    "                    )\n",
    "                    \n",
    "                    # Store the parameters in the dictionary\n",
    "                    params_dict[(animal_number, lactation_number)] = {'a': popt[0], 'b': popt[1], 'c': popt[2], 'd': popt[3]}\n",
    "                    \n",
    "                    # Predict the expected yield using the fitted model\n",
    "                    dataset.loc[group.index, 'ExpectedYield'] = wilmink_lactation_curve(group['DaysInMilk'], *popt)\n",
    "                \n",
    "                except OptimizeWarning:\n",
    "                    print(f\"OptimizeWarning for cow {animal_number}, lactation {lactation_number}, skipping.\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Curve fit failed for cow {animal_number}, lactation {lactation_number}: {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Value error for cow {animal_number}, lactation {lactation_number}: {e}\")\n",
    "    \n",
    "    # Keep only valid indices\n",
    "    dataset = dataset.loc[valid_indices].reset_index(drop=True)\n",
    "    \n",
    "    return dataset, params_dict\n",
    "\n",
    "# Apply the curve fitting function to your dataset\n",
    "data, params_dict = fit_wilmink_lactation_curve(data)\n",
    "\n",
    "# Remove rows where ExpectedYield is NaN\n",
    "data = data.dropna(subset=['ExpectedYield'])\n",
    "\n",
    "# Calculate NormalizedDailyYield, PreviousDailyYield, DailyYieldChange, and NormalizedDailyYieldChange\n",
    "data.loc[:, 'NormalizedDailyYield'] = data['DailyYield'] / data['ExpectedYield']\n",
    "data.loc[:, 'PreviousDailyYield'] = data.groupby('SE_Number')['DailyYield'].shift(1)\n",
    "data.loc[:, 'DailyYieldChange'] = data['DailyYield'] - data['PreviousDailyYield']\n",
    "data.loc[:, 'NormalizedDailyYieldChange'] = data['DailyYieldChange'] / data['ExpectedYield']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For plotting actual vs expected milk yield with THI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_Number = [\"SE-5c06d92d-3205\"] #[\"SE-f454e660-0710\"]  # [\"SE-5c06d92d-3183\"] # [\"SE-5b581702-1927\"] # [\"SE-a66dc90e-1614\"]\n",
    "cow_data = data[data[\"SE_Number\"].isin(SE_Number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY x MEANTHI\n",
    "\n",
    "# cow_data=data2.copy()\n",
    "cow_data = cow_data.sort_values(by='Date')\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot the first y-axis (Milk Yield)\n",
    "ax1.plot(cow_data['Date'], cow_data['ExpectedYield'],\n",
    "         label='Expected Milk Yield per Cow', color='skyblue', linestyle='--')\n",
    "ax1.plot(cow_data['Date'], cow_data['DailyYield'],\n",
    "         label='Actual Milk Yield per Cow', color='salmon', linestyle='-', markersize=1)\n",
    "\n",
    "ax1.set_xlabel('Date', fontsize=15)\n",
    "ax1.set_ylabel('Daily Milk Yield, liters per day', fontsize=15)\n",
    "ax1.tick_params(axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Daily Milk Yield With Expected Milk Yield and Mean THI, \\nSE-5c06d92d-3205', fontsize=20)\n",
    "# plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator())\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "# Create a second y-axis that shares the same x-axis\n",
    "ax2 = ax1.twinx() # instantiate a second Axes that shares the same x-axis\n",
    "ax2.plot(cow_data['Date'], cow_data['MeanTHI_adj'], color='g')\n",
    "ax2.set_ylabel('Mean THI', fontsize=15)\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with Wilminks program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the THI threshold ====================================================================================================>>> Change threshold here to 61 alt 67\n",
    "THI_THRESHOLD = 67\n",
    "\n",
    "# Calculate the daily heat load based on the THI threshold\n",
    "data['HeatLoad'] = data['MeanTHI_adj'].apply(lambda x: x - THI_THRESHOLD if x > THI_THRESHOLD else -(THI_THRESHOLD - x))\n",
    "\n",
    "# Initialize the cumulative heat load column with float type\n",
    "data['CumulativeHeatLoad'] = 0.0  # Explicitly set as float\n",
    "\n",
    "# Iterate through the data to calculate cumulative heat load correctly\n",
    "for i in range(1, len(data)):\n",
    "    previous_cumulative = data.at[i-1, 'CumulativeHeatLoad']\n",
    "    current_heat_load = data.at[i, 'HeatLoad']\n",
    "    \n",
    "    if current_heat_load < 0:  # If current heat load is negative\n",
    "        new_cumulative = previous_cumulative + 2 * current_heat_load\n",
    "    else:\n",
    "        new_cumulative = previous_cumulative + current_heat_load\n",
    "    \n",
    "    # Ensure the cumulative heat load never goes below zero\n",
    "    if new_cumulative > 0:\n",
    "        data.at[i, 'CumulativeHeatLoad'] = new_cumulative\n",
    "    else:\n",
    "        data.at[i, 'CumulativeHeatLoad'] = 0.0  # Ensure float is maintained\n",
    "\n",
    "data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When CumulativeHeatLoad is greater than 3, it indicates that the cow is under heat stress\n",
    "data['HeatStress'] = (data['CumulativeHeatLoad'] > 3).astype(int)\n",
    "data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe from the parameters dictionary, it should contain Se_Number, LactationNumber, a, b, c, d\n",
    "params_df = pd.DataFrame(params_dict).T.reset_index()\n",
    "params_df.columns = ['SE_Number', 'LactationNumber', 'a', 'b', 'c', 'd']\n",
    "params_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for each parameter\n",
    "params_df['z_a'] = zscore(params_df['a'])\n",
    "params_df['z_b'] = zscore(params_df['b'])\n",
    "params_df['z_c'] = zscore(params_df['c'])\n",
    "params_df['z_d'] = zscore(params_df['d'])\n",
    "\n",
    "params_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers (using Z-score > 3.5 or < -3.5 as threshold)\n",
    "outliers = params_df[(np.abs(params_df[['z_a', 'z_b', 'z_c', 'z_d']]) > 3.5).any(axis=1)]\n",
    "\n",
    "x = outliers.count()\n",
    "print(\"Number of outliers:\", x)\n",
    "\n",
    "# Optionally, drop the outliers\n",
    "params_df_cleaned = params_df.drop(outliers.index)\n",
    "params_df_cleaned.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique SE_Number and LactationNumber combinations from the outliers\n",
    "outlier_combinations = outliers[['SE_Number', 'LactationNumber']].drop_duplicates()\n",
    "\n",
    "# Merge with the original data to find rows that match these outlier combinations\n",
    "data_cleaned = data.merge(outlier_combinations, on=['SE_Number', 'LactationNumber'], how='left', indicator=True)\n",
    "\n",
    "# Keep only the rows that do not match the outlier combinations\n",
    "data_cleaned = data_cleaned[data_cleaned['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Now data_cleaned contains the original data with the outlier combinations removed\n",
    "print(\"Number of rows removed:\", len(data) - len(data_cleaned))\n",
    "data_cleaned.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned['Residuals'] = data_cleaned['DailyYield'] - data_cleaned['ExpectedYield']\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'FarmName_Pseudo' to perform calculations at the farm level\n",
    "farm_results = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    farm_residuals = []\n",
    "    \n",
    "    for se_number, cow_group in farm_group.groupby('SE_Number'):\n",
    "        residuals = cow_group['Residuals']\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            farm_residuals.append(residuals)\n",
    "    \n",
    "    if len(farm_residuals) > 0:\n",
    "        # Combine residuals from all cows in the farm\n",
    "        combined_residuals = np.concatenate(farm_residuals)\n",
    "        \n",
    "        # Calculate farm-level statistics\n",
    "        mean_residuals = combined_residuals.mean()\n",
    "        std_residuals = combined_residuals.std()\n",
    "        acf_values = acf(combined_residuals, nlags=30, fft=False)\n",
    "        pacf_values = pacf(combined_residuals, nlags=min(30, len(combined_residuals)//2))\n",
    "\n",
    "        # Print the farm-level statistics\n",
    "        print(f\"Farm: {farm_name}\")\n",
    "        print(f\"Mean Residuals: {mean_residuals}\")\n",
    "        print(f\"Standard Deviation of Residuals: {std_residuals}\")\n",
    "        print(f\"ACF (first 5 lags): {acf_values[:5]}\")\n",
    "        print(f\"PACF (first 5 lags): {pacf_values[:5]}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'FarmName_Pseudo', 'SE_Number', and 'LactationNumber' to perform individual calculations\n",
    "farm_results = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    print(f\"Farm: {farm_name}\")\n",
    "    \n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals']\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            mean_residuals = residuals.mean()\n",
    "            std_residuals = residuals.std()\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Print the statistics\n",
    "            print(f\"\\nCow: {se_number}, Lactation Number: {lactation_number}\")\n",
    "            print(f\"Mean Residuals: {mean_residuals}\")\n",
    "            print(f\"Standard Deviation of Residuals: {std_residuals}\")\n",
    "            print(f\"ACF (first 5 lags): {acf_values[:5]}\")\n",
    "            print(f\"PACF (first 5 lags): {pacf_values[:5]}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "mean_residual_threshold = 0.075\n",
    "std_residual_threshold = 7.5\n",
    "acf_threshold = 0.25\n",
    "pacf_threshold = 0.25\n",
    "\n",
    "# List to collect flagged combinations\n",
    "flagged_combinations = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals'].dropna()\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            mean_residuals = residuals.mean()\n",
    "            std_residuals = residuals.std()\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Check against thresholds\n",
    "            if (abs(mean_residuals) > mean_residual_threshold or \n",
    "                std_residuals > std_residual_threshold or \n",
    "                abs(acf_values[1]) > acf_threshold or \n",
    "                abs(pacf_values[1]) > pacf_threshold):\n",
    "                \n",
    "                # Collect the combination if it exceeds any threshold\n",
    "                flagged_combinations.append({\n",
    "                    'Farm': farm_name,\n",
    "                    'SE_Number': se_number,\n",
    "                    'LactationNumber': lactation_number,\n",
    "                    'Mean Residuals': mean_residuals,\n",
    "                    'Std Residuals': std_residuals,\n",
    "                    'ACF[1]': acf_values[1],\n",
    "                    'PACF[1]': pacf_values[1]\n",
    "                })\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "flagged_df = pd.DataFrame(flagged_combinations)\n",
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_cleaned.shape)\n",
    "print(data_cleaned[['SE_Number', 'LactationNumber']].drop_duplicates())\n",
    "print(flagged_df[['SE_Number', 'LactationNumber']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOAKIM'S EDITS\n",
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    dim = np.array(dim, dtype=float)\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to directly refit the Wilmink Lactation Curve (Standard Process)\n",
    "def refit_wilmink(cow_data):\n",
    "    x_data = cow_data['DaysInMilk'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Use initial guesses and bounds from the original fitting process\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "    popt, _ = curve_fit(wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=30000)\n",
    "    \n",
    "    # Calculate the expected yield with the refitted parameters\n",
    "    cow_data['ExpectedYield'] = wilmink_lactation_curve(cow_data['DaysInMilk'], *popt)\n",
    "    \n",
    "    # Calculate new residuals\n",
    "    cow_data['Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Define the Robust Wilmink Lactation Curve function\n",
    "def robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3):\n",
    "    dim = np.array(dim, dtype=np.float64)\n",
    "    days_in_milk = dim[0]\n",
    "    lag_1 = dim[1]\n",
    "    lag_2 = dim[2]\n",
    "    lag_3 = dim[3]\n",
    "    \n",
    "    return a + b * days_in_milk + c * np.exp(-d * days_in_milk) + lag1 * lag_1 + lag2 * lag_2 + lag3 * lag_3\n",
    "\n",
    "def fit_robust_wilmink(cow_data, lags=3):\n",
    "    cow_data = add_lagged_variables(cow_data, max_lag=lags)\n",
    "\n",
    "    # Extract individual columns from cow_data as separate arrays\n",
    "    days_in_milk = cow_data['DaysInMilk'].values\n",
    "    lag_1 = cow_data['lag_1'].values\n",
    "    lag_2 = cow_data['lag_2'].values\n",
    "    lag_3 = cow_data['lag_3'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Ensure all arrays have the same shape\n",
    "    assert len(days_in_milk) == len(lag_1) == len(lag_2) == len(lag_3) == len(y_data), \"Mismatch in data lengths\"\n",
    "\n",
    "    # Prepare initial guesses and bounds\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1, 0, 0, 0]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0, -np.inf, -np.inf, -np.inf], \n",
    "              [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "    \n",
    "    try:\n",
    "        # Pass individual components of x_data to curve_fit\n",
    "        popt, _ = curve_fit(\n",
    "            lambda dim, a, b, c, d, lag1, lag2, lag3: robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3), \n",
    "            (days_in_milk, lag_1, lag_2, lag_3), \n",
    "            y_data, \n",
    "            p0=initial_guesses, \n",
    "            bounds=bounds, \n",
    "            maxfev=50000\n",
    "        )\n",
    "\n",
    "        cow_data.loc[:, 'ExpectedYield'] = robust_wilmink_lactation_curve(\n",
    "            (days_in_milk, lag_1, lag_2, lag_3), *popt\n",
    "        )\n",
    "        cow_data.loc[:, 'Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Curve fitting failed: {e}\")\n",
    "        cow_data['ExpectedYield'] = np.nan\n",
    "        cow_data['Residuals'] = np.nan\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    \n",
    "    # Check for missing values and drop rows with NaNs in the lagged columns or DailyYield\n",
    "    cow_data_cleaned = cow_data.dropna(subset=['DailyYield'] + [f'lag_{lag}' for lag in range(1, max_lag + 1)])\n",
    "    \n",
    "    # Ensure we're not dropping too much data, and there's still sufficient data for fitting\n",
    "    if len(cow_data_cleaned) == 0:\n",
    "        raise ValueError(\"Insufficient data after adding lagged variables. Check for missing data.\")\n",
    "\n",
    "    return cow_data_cleaned\n",
    "\n",
    "\n",
    "# Apply lagged variables to all cases, all lactations\n",
    "for se_number in data_cleaned['SE_Number'].unique():\n",
    "    for lactation_number in data_cleaned[data_cleaned['SE_Number'] == se_number]['LactationNumber'].unique():\n",
    "        \n",
    "        cow_data = data_cleaned[(data_cleaned['SE_Number'] == se_number) & \n",
    "                                (data_cleaned['LactationNumber'] == lactation_number)].copy()\n",
    "        \n",
    "        # Apply lagged variables for all cases, regardless of autocorrelation\n",
    "        cow_data = add_lagged_variables(cow_data, max_lag=3)\n",
    "        cow_data_refitted = fit_robust_wilmink(cow_data, lags=3)\n",
    "        \n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "\n",
    "# Remove rows where ExpectedYield is NaN\n",
    "data_cleaned = data_cleaned.dropna(subset=['ExpectedYield']).reset_index(drop=True)\n",
    "\n",
    "# Normalize yields\n",
    "data_cleaned['NormalizedDailyYield'] = data_cleaned['DailyYield'] / data_cleaned['ExpectedYield']\n",
    "data_cleaned['NormalizedDailyYieldChange'] = data_cleaned['DailyYieldChange'] / data_cleaned['ExpectedYield']\n",
    "\n",
    "\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# OLD CODE\n",
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    dim = np.array(dim, dtype=float)\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to directly refit the Wilmink Lactation Curve (Standard Process)\n",
    "def refit_wilmink(cow_data):\n",
    "    x_data = cow_data['DaysInMilk'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Use initial guesses and bounds from the original fitting process\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "    popt, _ = curve_fit(wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=30000)\n",
    "    \n",
    "    # Calculate the expected yield with the refitted parameters\n",
    "    cow_data['ExpectedYield'] = wilmink_lactation_curve(cow_data['DaysInMilk'], *popt)\n",
    "    \n",
    "    # Calculate new residuals\n",
    "    cow_data['Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Define the Robust Wilmink Lactation Curve function\n",
    "def robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3):\n",
    "    dim = np.array(dim, dtype=np.float64)\n",
    "    days_in_milk = dim[0]\n",
    "    lag_1 = dim[1]\n",
    "    lag_2 = dim[2]\n",
    "    lag_3 = dim[3]\n",
    "    \n",
    "    return a + b * days_in_milk + c * np.exp(-d * days_in_milk) + lag1 * lag_1 + lag2 * lag_2 + lag3 * lag_3\n",
    "\n",
    "# Function to fit the robust Wilmink model\n",
    "def fit_robust_wilmink(cow_data, lags=3):\n",
    "    cow_data = add_lagged_variables(cow_data, max_lag=lags)\n",
    "    \n",
    "    x_data = cow_data[['DaysInMilk', 'lag_1', 'lag_2', 'lag_3']].values.T\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "    \n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1, 0, 0, 0]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0, -np.inf, -np.inf, -np.inf], \n",
    "              [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(robust_wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=50000)\n",
    "        cow_data.loc[:, 'ExpectedYield'] = robust_wilmink_lactation_curve(x_data, *popt)\n",
    "        cow_data.loc[:, 'Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Curve fitting failed: {e}\")\n",
    "        cow_data.loc[:, 'ExpectedYield'] = np.nan\n",
    "        cow_data.loc[:, 'Residuals'] = np.nan\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Example usage: Applying the robust model to flagged cases\n",
    "for index, row in flagged_df.iterrows():\n",
    "    se_number = row['SE_Number']\n",
    "    lactation_number = row['LactationNumber']\n",
    "    \n",
    "    cow_data = data_cleaned[(data_cleaned['SE_Number'] == se_number) & \n",
    "                            (data_cleaned['LactationNumber'] == lactation_number)].copy()\n",
    "    \n",
    "    if abs(row['ACF[1]']) > 0.2:  # Significant autocorrelation\n",
    "        cow_data = add_lagged_variables(cow_data, max_lag=3)\n",
    "        cow_data_refitted = fit_robust_wilmink(cow_data, lags=3)\n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "    else:\n",
    "        cow_data_refitted = refit_wilmink(cow_data)\n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "\n",
    "# Erase all rows where ExpectedYield is NaN\n",
    "data_cleaned = data_cleaned.dropna(subset=['ExpectedYield']).reset_index(drop=True)\n",
    "\n",
    "data_cleaned['NormalizedDailyYield'] = data_cleaned['DailyYield'] / data_cleaned['ExpectedYield']\n",
    "data_cleaned['NormalizedDailyYieldChange'] = data_cleaned['DailyYieldChange'] / data_cleaned['ExpectedYield']\n",
    "\n",
    "data_cleaned\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "mean_residual_threshold = 0.075\n",
    "std_residual_threshold = 7.5\n",
    "acf_threshold = 0.25\n",
    "pacf_threshold = 0.25\n",
    "\n",
    "# List to collect flagged combinations\n",
    "flagged_combinations = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals'].dropna()\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            mean_residuals = residuals.mean()\n",
    "            std_residuals = residuals.std()\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Check against thresholds\n",
    "            if (abs(mean_residuals) > mean_residual_threshold or \n",
    "                std_residuals > std_residual_threshold or \n",
    "                abs(acf_values[1]) > acf_threshold or \n",
    "                abs(pacf_values[1]) > pacf_threshold):\n",
    "                \n",
    "                # Collect the combination if it exceeds any threshold\n",
    "                flagged_combinations.append({\n",
    "                    'Farm': farm_name,\n",
    "                    'SE_Number': se_number,\n",
    "                    'LactationNumber': lactation_number,\n",
    "                    'Mean Residuals': mean_residuals,\n",
    "                    'Std Residuals': std_residuals,\n",
    "                    'ACF[1]': acf_values[1],\n",
    "                    'PACF[1]': pacf_values[1]\n",
    "                })\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "flagged_df = pd.DataFrame(flagged_combinations)\n",
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, threshold=3.5):\n",
    "    # Calculate z-scores of residuals\n",
    "    data = data.copy()  # Create a copy to avoid the SettingWithCopyWarning\n",
    "    data['z_score'] = (data['Residuals'] - data['Residuals'].mean()) / data['Residuals'].std()\n",
    "    \n",
    "    # Identify the number of outliers\n",
    "    num_outliers = (data['z_score'].abs() >= threshold).sum()\n",
    "    print(f\"Number of outliers detected: {num_outliers}\")\n",
    "    \n",
    "    # Remove rows where the z-score of the residual is greater than the threshold\n",
    "    cleaned_data = data.loc[(data['z_score'].abs() < threshold)].drop(columns=['z_score'])\n",
    "    \n",
    "    # Print the number of rows before and after\n",
    "    print(f\"Number of rows before outlier removal: {len(data)}\")\n",
    "    print(f\"Number of rows after outlier removal: {len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Apply to flagged cases\n",
    "for index, row in flagged_df.iterrows():\n",
    "    se_number = row['SE_Number']\n",
    "    lactation_number = row['LactationNumber']\n",
    "    \n",
    "    # Select the cow data for the specific SE_Number and LactationNumber\n",
    "    cow_data = data_cleaned.loc[(data_cleaned['SE_Number'] == se_number) & \n",
    "                                (data_cleaned['LactationNumber'] == lactation_number)]\n",
    "    \n",
    "    # Remove outliers\n",
    "    cow_data_trimmed = remove_outliers(cow_data, threshold=3.5)\n",
    "    \n",
    "    # Recalculate the residuals and update the dataset\n",
    "    cow_data_trimmed['Residuals'] = cow_data_trimmed['DailyYield'] - cow_data_trimmed['ExpectedYield']\n",
    "    \n",
    "    # Remove the old data for this cow from data_cleaned\n",
    "    data_cleaned = data_cleaned.loc[~((data_cleaned['SE_Number'] == se_number) & \n",
    "                                      (data_cleaned['LactationNumber'] == lactation_number))]\n",
    "    \n",
    "    # Append the cleaned data back to data_cleaned\n",
    "    data_cleaned = pd.concat([data_cleaned, cow_data_trimmed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "new_order = [\n",
    "    \"Date\", \"FarmName_Pseudo\", \"SE_Number\", \"Age\", \"BreedName\", \"LactationNumber\", \"DaysInMilk\",'YearSeason', \"DailyYield\", \"PreviousDailyYield\", \n",
    "    \"DailyYieldChange\", \"ExpectedYield\", \"NormalizedDailyYield\", \n",
    "    \"NormalizedDailyYieldChange\", \"Residuals\", \"HeatStress\", \"Temp15Threshold\", \"HW\", \n",
    "    \"cum_HW\", \"MeanTemperature\", \"MeanTHI_adj\", \"HeatLoad\", \"CumulativeHeatLoad\"\n",
    "]\n",
    "data_cleaned = data_cleaned[new_order]\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of HeatStress occurrences in each farm\n",
    "heat_stress_counts = data_cleaned.groupby('FarmName_Pseudo')['HeatStress'].sum()\n",
    "heat_stress_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of observations within each farm\n",
    "no_obs = data_cleaned.groupby('FarmName_Pseudo').size().reset_index(name='count')\n",
    "no_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reordered DataFrame to a CSV file - 61 degrees threshold\n",
    "# data_cleaned.to_csv('../Data/MergedData/HeatApproachCleanedYieldDataTest61.csv', index=False)\n",
    "# print(data_cleaned.shape)\n",
    "\n",
    "# 67 degrees threshold\n",
    "data_cleaned.to_csv('../Data/MergedData/HeatApproachCleanedYieldDataTest67.csv', index=False)\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics\n",
    "- After Wilminks program using unfiltered data\n",
    "- After Wilminks program using filtered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If attempting filtering after Wilminks instead of before\n",
    "df3 = pd.read_csv('../Data/MergedData/HeatApproachYieldDataTest.csv', low_memory=False)\n",
    "\n",
    "df3 = df3[(df3[\"BreedName\"] == \"NRDC\") |\n",
    "          (df3[\"BreedName\"] == \"SLB\") |\n",
    "          (df3[\"BreedName\"] == \"DairyCross\") |\n",
    "          (df3[\"BreedName\"] == \"SJB\")]\n",
    "\n",
    "print(df3.shape)\n",
    "\n",
    "# Keep only lactation 1-7\n",
    "df_lact = df3[df3[\"LactationNumber\"] <= 8]\n",
    "print(f\"No. milking events in SRB, SH, SJB and dairy crosses cows within 1-40 DIM and 100-400 DIM in lactation 1-8: \"\n",
    "      f\"{df_lact.shape}\")  \n",
    "\n",
    "# No. lactations and cows in data\n",
    "test = df_lact.drop_duplicates(subset=[\"SE_Number\", \"LactationNumber\"])\n",
    "print(f\"No. of lactations from SRB, SH, SJB and dairy crosses within 1-40DIM and 100-400DIM in lactation 1-8: {test.shape}\")\n",
    "test = df_lact.drop_duplicates(subset=[\"SE_Number\"])\n",
    "print(f\"No. of cows from SRB, SH, SJB and dairy crosses within 1-40DIM and 100-400DIM in lactation 1-8: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Parity 1-3\n",
    "df_lact = df_lact.copy()\n",
    "df_lact[\"Parity\"] = df_lact[\"LactationNumber\"]\n",
    "df_lact.loc[(df_lact['LactationNumber'] >= 3) & (df_lact['LactationNumber'] <= 8), 'Parity'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# DESCRIPTIVE STATISTICS\n",
    "# By parity\n",
    "for_my_rec5 = df_lact.drop_duplicates(subset=[\"SE_Number\", \"LactationNumber\"])\n",
    "print(f\"No. of parities in milking file: {for_my_rec5.shape}\")  # 1,207\n",
    "\n",
    "count_my_rec = for_my_rec5.groupby([\"Parity\", \"BreedName\"])[\"SE_Number\"].count().reset_index()\n",
    "print(f\"No. of parities from SRB, SH, SJB and dairy crosses: \\n\", count_my_rec.to_string(index=False))\n",
    "\n",
    "# By cows\n",
    "for_my_rec4 = df_lact.drop_duplicates(subset=[\"SE_Number\"])\n",
    "print(f\"No. of cows in milking file: {for_my_rec4.shape}\")  # 921\n",
    "\n",
    "for_my_rec5 = for_my_rec4.drop_duplicates(subset=[\"SE_Number\"])\n",
    "count_my_rec = for_my_rec5.groupby([\"BreedName\"])[\"SE_Number\"].count().reset_index()\n",
    "print(f\"No. of cows from SRB, SH, SJB and dairy crosses: \\n\", count_my_rec.to_string(index=False))\n",
    "\n",
    "# Herd info\n",
    "# df_lact = pd.read_csv(\"../Data/MY_weather_filtered.csv\", low_memory=False)\n",
    "df_lact = df_lact.drop_duplicates(subset=[\"FarmName_Pseudo\"])\n",
    "col_keep = [\"FarmName_Pseudo\"]\n",
    "df_lact = df_lact[col_keep]\n",
    "print(df_lact.shape)\n",
    "print(f\"Herds in filtered data: \\n\", df_lact.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reordered DataFrame to a CSV file\n",
    "data_cleaned.to_csv('../Data/MergedData/HeatApproachCleanedYieldDataTest.csv', index=False)\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTIVE STATISTICS FILTERED DATA\n",
    "df_lact = pd.read_csv('../Data/MergedData/HeatApproachCleanedYieldDataTest67.csv', low_memory=False)\n",
    "\n",
    "# Make Parity 1-3\n",
    "df_lact = df_lact.copy()\n",
    "df_lact[\"Parity\"] = df_lact[\"LactationNumber\"]\n",
    "df_lact.loc[(df_lact['LactationNumber'] >= 3) & (df_lact['LactationNumber'] <= 8), 'Parity'] = 3\n",
    "\n",
    "# By parity\n",
    "for_my_rec5 = df_lact.drop_duplicates(subset=[\"SE_Number\", \"LactationNumber\"])\n",
    "print(f\"No. of parities in milking file: {for_my_rec5.shape}\") \n",
    "\n",
    "count_my_rec = for_my_rec5.groupby([\"Parity\", \"BreedName\"])[\"SE_Number\"].count().reset_index()\n",
    "print(f\"No. of parities from SRB, SH, SJB and dairy crosses: \\n\", count_my_rec.to_string(index=False))\n",
    "\n",
    "# By cows\n",
    "for_my_rec4 = df_lact.drop_duplicates(subset=[\"SE_Number\"])\n",
    "print(f\"No. of cows in milking file: {for_my_rec4.shape}\") \n",
    "\n",
    "for_my_rec5 = for_my_rec4.drop_duplicates(subset=[\"SE_Number\"])\n",
    "count_my_rec = for_my_rec5.groupby([\"BreedName\"])[\"SE_Number\"].count().reset_index()\n",
    "print(f\"No. of cows from SRB, SH, SJB and dairy crosses: \\n\", count_my_rec.to_string(index=False))\n",
    "\n",
    "# Herd info\n",
    "df_lact = df_lact.drop_duplicates(subset=[\"FarmName_Pseudo\"])\n",
    "col_keep = [\"FarmName_Pseudo\"]\n",
    "df_lact = df_lact[col_keep]\n",
    "print(df_lact.shape)\n",
    "print(f\"Herds in filtered data: \\n\", df_lact.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
