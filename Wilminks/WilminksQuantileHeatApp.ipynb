{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit, OptimizeWarning\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from scipy.optimize import minimize\n",
    "from vqr import VectorQuantileRegressor\n",
    "from vqr.solvers.regularized_lse import RegularizedDualVQRSolver\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'FarmName_Pseudo': 'str',\n",
    "    'SE_Number': 'str',\n",
    "    'AnimalNumber': 'Int64',          \n",
    "    'StartDate': 'str',\n",
    "    'StartTime': 'str',\n",
    "    'DateTime': 'str',\n",
    "    'LactationNumber': 'Int64',       \n",
    "    'DaysInMilk': 'Int64', \n",
    "    'YearSeason': 'str',           \n",
    "    'TotalYield': 'float',\n",
    "    'DateTime': 'str',\n",
    "    'BreedName': 'str',\n",
    "    'Age': 'Int64',\n",
    "    'Mother': 'str',\n",
    "    'Father': 'str',\n",
    "    'CullDecisionDate': 'str',\n",
    "    'Temperature': 'float',\n",
    "    'RelativeHumidity': 'float',      \n",
    "    'THI_adj': 'float',\n",
    "    'HW': 'Int64',                    \n",
    "    'cum_HW': 'Int64',                \n",
    "    'Temp15Threshold': 'Int64'        \n",
    "}\n",
    "\n",
    "\n",
    "# Load the CSV with specified dtypes\n",
    "data = pd.read_csv('../Data/MergedData/CleanedYieldData.csv', dtype=dtype_dict)\n",
    "\n",
    "# Convert date and time columns back to datetime and time objects\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'], errors='coerce')\n",
    "data['StartTime'] = pd.to_datetime(data['StartTime'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "data['CullDecisionDate'] = pd.to_datetime(data['CullDecisionDate'], errors='coerce')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'], errors='coerce')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the DailyYield for each cow each day\n",
    "data['DailyYield'] = data.groupby(['SE_Number', 'StartDate'])['TotalYield'].transform('sum')\n",
    "\n",
    "# Sort the data by AnimalNumber and StartDate\n",
    "data.sort_values(['AnimalNumber', 'StartDate'], inplace=True)\n",
    "\n",
    "# Calculate the previous day's total yield for each cow\n",
    "data['PreviousDailyYield'] = data.groupby('AnimalNumber')['DailyYield'].shift(1)\n",
    "\n",
    "# Calculate the daily yield change for each cow\n",
    "data['DailyYieldChange'] = data['DailyYield'] - data['PreviousDailyYield']\n",
    "\n",
    "# Group and aggregate data ======================================================================>>> if running with filtered data: change Temperature to Temperature2 and THI to THI_adj2\n",
    "data = data.groupby(['SE_Number', 'FarmName_Pseudo', 'StartDate']).agg({\n",
    "    'DailyYield': 'first',\n",
    "    'PreviousDailyYield': 'first',\n",
    "    'DailyYieldChange': 'first',\n",
    "    'HW': 'max',\n",
    "    'Temperature2': 'mean',\n",
    "    'THI_adj2': 'mean',\n",
    "    'DaysInMilk': 'first',\n",
    "    'YearSeason': 'first',\n",
    "    'cum_HW': 'max',\n",
    "    'Temp15Threshold': 'max',\n",
    "    'Age': 'first',\n",
    "    'BreedName': 'first',\n",
    "    'LactationNumber': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Renaming and formatting ======================================================================>>> if running with filtered data: change Temperature to Temperature2 and THI to THI_adj2\n",
    "data.rename(columns={\n",
    "    'Temperature2': 'MeanTemperature',\n",
    "    'THI_adj2': 'MeanTHI_adj',\n",
    "    'StartDate': 'Date'\n",
    "}, inplace=True)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if DailyYield is centered around approx the same for each farm\n",
    "print(\"Mean of DailyYield:\", data.groupby('FarmName_Pseudo')['DailyYield'].mean())\n",
    "print(\"Standard Deviation of DailyYield:\", data.groupby('FarmName_Pseudo')['DailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to remove outliers\n",
    "def remove_outliers(group, threshold=3.5):\n",
    "    mean = np.mean(group['DailyYield'])\n",
    "    std_dev = np.std(group['DailyYield'])\n",
    "    return group[(group['DailyYield'] > mean - threshold * std_dev) & (group['DailyYield'] < mean + threshold * std_dev)]\n",
    "\n",
    "# Function to smooth the data using .loc to avoid SettingWithCopyWarning\n",
    "def smooth_data(group, window=5):\n",
    "    group.loc[:, 'DailyYield'] = group['DailyYield'].rolling(window, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "# Function to fit curve_fit before applying Quantile Regression\n",
    "def fit_with_curve_fit_before_quantreg(dataset, quantile=0.7, max_iter=100000):\n",
    "    params_dict = {}\n",
    "    valid_indices = []\n",
    "\n",
    "    for (animal_number, lactation_number), group in tqdm(dataset.groupby(['SE_Number', 'LactationNumber']), unit=\" Segments\"):\n",
    "        try:\n",
    "            group = remove_outliers(group)\n",
    "            group = smooth_data(group)\n",
    "            x_data = group['DaysInMilk'].values.astype(float)\n",
    "            y_data = group['DailyYield'].values.astype(float)\n",
    "\n",
    "            # Ensure there are enough data points to fit the curve\n",
    "            if (len(x_data) < 150) or (len(y_data) < 150):\n",
    "                print(f\"Insufficient data points for cow {animal_number}, lactation {lactation_number}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            valid_indices.extend(group.index)\n",
    "\n",
    "            # Fit the model using curve_fit\n",
    "            try:\n",
    "                # Initial parameter guesses\n",
    "                initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "                # Bounds on the parameters to prevent overflow\n",
    "                bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('error', category=OptimizeWarning)\n",
    "                    popt, _ = curve_fit(\n",
    "                        wilmink_lactation_curve, x_data, y_data,\n",
    "                        p0=initial_guesses, bounds=bounds, maxfev=30000\n",
    "                    )\n",
    "\n",
    "                # Store the parameters in the dictionary\n",
    "                params_dict[(animal_number, lactation_number)] = {'a': popt[0], 'b': popt[1], 'c': popt[2], 'd': popt[3]}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Curve fitting failed for cow {animal_number}, lactation {lactation_number}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Now use the parameters from curve_fit for quantile regression\n",
    "            X = np.column_stack([np.ones_like(x_data), x_data, np.exp(-x_data), -x_data * np.exp(-x_data)])\n",
    "            quantreg_model = sm.QuantReg(y_data, X)\n",
    "            quantreg_fit = quantreg_model.fit(q=quantile, max_iter=max_iter, start_params=popt)\n",
    "\n",
    "            # Update parameters after quantile regression\n",
    "            a, b, c, d = quantreg_fit.params\n",
    "            dataset.loc[group.index, 'ExpectedYield'] = wilmink_lactation_curve(group['DaysInMilk'], a, b, c, d)\n",
    "            params_dict[(animal_number, lactation_number)] = {'a': a, 'b': b, 'c': c, 'd': d}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cow {animal_number}, lactation {lactation_number}: {e}\")\n",
    "\n",
    "    return dataset, params_dict\n",
    "\n",
    "# Apply the curve fitting before quantile regression\n",
    "data, params_dict = fit_with_curve_fit_before_quantreg(data, quantile=0.7, max_iter=100000)\n",
    "\n",
    "# Remove rows where ExpectedYield is NaN\n",
    "data = data.dropna(subset=['ExpectedYield'])\n",
    "\n",
    "# Calculate NormalizedDailyYield, PreviousDailyYield, DailyYieldChange, and NormalizedDailyYieldChange\n",
    "data.loc[:, 'NormalizedDailyYield'] = data['DailyYield'] / data['ExpectedYield']\n",
    "data.loc[:, 'PreviousDailyYield'] = data.groupby('SE_Number')['DailyYield'].shift(1)\n",
    "data.loc[:, 'DailyYieldChange'] = data['DailyYield'] - data['PreviousDailyYield']\n",
    "data.loc[:, 'NormalizedDailyYieldChange'] = data['DailyYieldChange'] / data['ExpectedYield']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the THI threshold ================================================================================>>> Change threshold here to 61 alt 67\n",
    "THI_THRESHOLD = 67\n",
    "\n",
    "# Calculate the daily heat load based on the THI threshold\n",
    "data['HeatLoad'] = data['MeanTHI_adj'].apply(lambda x: x - THI_THRESHOLD if x > THI_THRESHOLD else -(THI_THRESHOLD - x))\n",
    "\n",
    "# Initialize the cumulative heat load column with float type\n",
    "data['CumulativeHeatLoad'] = 0.0  # Explicitly set as float\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Iterate through the data to calculate cumulative heat load correctly\n",
    "for i in range(1, len(data)):\n",
    "    previous_cumulative = data.at[i-1, 'CumulativeHeatLoad']\n",
    "    current_heat_load = data.at[i, 'HeatLoad']\n",
    "    \n",
    "    if current_heat_load < 0:  # If current heat load is negative\n",
    "        new_cumulative = previous_cumulative + 2 * current_heat_load\n",
    "    else:\n",
    "        new_cumulative = previous_cumulative + current_heat_load\n",
    "    \n",
    "    # Ensure the cumulative heat load never goes below zero\n",
    "    if new_cumulative > 0:\n",
    "        data.at[i, 'CumulativeHeatLoad'] = new_cumulative\n",
    "    else:\n",
    "        data.at[i, 'CumulativeHeatLoad'] = 0.0  # Ensure float is maintained\n",
    "\n",
    "# Drop rows where the 'DailyYield' column has NaN values\n",
    "data = data.dropna(subset=['DailyYield'])\n",
    "\n",
    "data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When CumulativeHeatLoad is greater than 3, it indicates that the cow is under heat stress\n",
    "data['HeatStress'] = (data['CumulativeHeatLoad'] > 3).astype(int)\n",
    "data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe from the parameters dictionary, it should contain Se_Number, LactationNumber, a, b, c, d\n",
    "params_df = pd.DataFrame(params_dict).T.reset_index()\n",
    "params_df.columns = ['SE_Number', 'LactationNumber', 'a', 'b', 'c', 'd']\n",
    "params_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for each parameter\n",
    "params_df['z_a'] = zscore(params_df['a'])\n",
    "params_df['z_b'] = zscore(params_df['b'])\n",
    "params_df['z_c'] = zscore(params_df['c'])\n",
    "params_df['z_d'] = zscore(params_df['d'])\n",
    "\n",
    "params_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers (using Z-score > 3.5 or < -3.5 as threshold)\n",
    "outliers = params_df[(np.abs(params_df[['z_a', 'z_b', 'z_c', 'z_d']]) > 3.5).any(axis=1)]\n",
    "\n",
    "x = outliers.count()\n",
    "print(\"Number of outliers:\", x)\n",
    "\n",
    "# Optionally, drop the outliers\n",
    "params_df_cleaned = params_df.drop(outliers.index)\n",
    "params_df_cleaned.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique SE_Number and LactationNumber combinations from the outliers\n",
    "outlier_combinations = outliers[['SE_Number', 'LactationNumber']].drop_duplicates()\n",
    "\n",
    "# Merge with the original data to find rows that match these outlier combinations\n",
    "data_cleaned = data.merge(outlier_combinations, on=['SE_Number', 'LactationNumber'], how='left', indicator=True)\n",
    "\n",
    "# Keep only the rows that do not match the outlier combinations\n",
    "data_cleaned = data_cleaned[data_cleaned['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Now data_cleaned contains the original data with the outlier combinations removed\n",
    "print(\"Number of rows removed:\", len(data) - len(data_cleaned))\n",
    "data_cleaned.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After discussion with L.M. Tamminen, run to here and use the resulting dataframes in GAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change order of columns\n",
    "col_keep = ['Date', 'FarmName_Pseudo', 'SE_Number', 'BreedName', 'LactationNumber', 'Age', 'DaysInMilk',\n",
    "            'DailyYield', 'ExpectedYield', 'PreviousDailyYield', 'DailyYieldChange', 'YearSeason', \n",
    "            'NormalizedDailyYield', 'NormalizedDailyYieldChange',\n",
    "            'HeatStress', 'Temp15Threshold', 'HW', 'cum_HW', 'MeanTemperature', 'MeanTHI_adj', 'HeatLoad', 'CumulativeHeatLoad']\n",
    "\n",
    "data_cleaned = data_cleaned[col_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "# data_cleaned.to_csv(\"../Data/MergedData/QuantileRerunTHI61.csv\", index=False)\n",
    "data_cleaned.to_csv(\"../Data/MergedData/QuantileRerunTHI67.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is uncertain \n",
    "We don't know for sure what the rest of the code actually does. Joakim refers to residuals, but it's actually just change in milk yield (DailyYield minus ExpectedYield) and not actual residuals form the regression model above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned['Residuals'] = data_cleaned['DailyYield'] - data_cleaned['ExpectedYield']\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_results = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    farm_residuals = []\n",
    "    \n",
    "    for se_number, cow_group in farm_group.groupby('SE_Number'):\n",
    "        residuals = cow_group['Residuals'].dropna()  # Drop NaN values\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            farm_residuals.append(residuals)\n",
    "    \n",
    "    if len(farm_residuals) > 0:\n",
    "        # Combine residuals from all cows in the farm\n",
    "        combined_residuals = np.concatenate(farm_residuals)\n",
    "        \n",
    "        if len(combined_residuals) > 1:  # Ensure enough data to perform calculations\n",
    "            # Calculate farm-level statistics\n",
    "            acf_values = acf(combined_residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(combined_residuals, nlags=min(30, len(combined_residuals)//2))\n",
    "\n",
    "            # Print the farm-level statistics\n",
    "            print(f\"Farm: {farm_name}\")\n",
    "            print(f\"ACF (first 5 lags): {acf_values[:5]}\")\n",
    "            print(f\"PACF (first 5 lags): {pacf_values[:5]}\")\n",
    "        else:\n",
    "            print(f\"Farm: {farm_name} does not have enough data for reliable calculations.\")\n",
    "        \n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'FarmName_Pseudo', 'SE_Number', and 'LactationNumber' to perform individual calculations\n",
    "farm_results = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    print(f\"Farm: {farm_name}\")\n",
    "    \n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals']\n",
    "        residuals = cow_group['Residuals'].dropna()  # Drop NaN values\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Print the statistics\n",
    "            print(f\"\\nCow: {se_number}, Lactation Number: {lactation_number}\")\n",
    "            print(f\"ACF (first 5 lags): {acf_values[:5]}\")\n",
    "            print(f\"PACF (first 5 lags): {pacf_values[:5]}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "mean_residual_threshold = 0.075\n",
    "std_residual_threshold = 7.5\n",
    "acf_threshold = 0.25\n",
    "pacf_threshold = 0.25\n",
    "\n",
    "# List to collect flagged combinations\n",
    "flagged_combinations = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals'].dropna()\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Check against thresholds\n",
    "            if (abs(acf_values[1]) > acf_threshold or \n",
    "                abs(pacf_values[1]) > pacf_threshold):\n",
    "                \n",
    "                # Collect the combination if it exceeds any threshold\n",
    "                flagged_combinations.append({\n",
    "                    'Farm': farm_name,\n",
    "                    'SE_Number': se_number,\n",
    "                    'LactationNumber': lactation_number,\n",
    "                    'ACF[1]': acf_values[1],\n",
    "                    'PACF[1]': pacf_values[1]\n",
    "                })\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "flagged_df = pd.DataFrame(flagged_combinations)\n",
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOAKIM'S EDITS\n",
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    dim = np.array(dim, dtype=float)\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to directly refit the Wilmink Lactation Curve (Standard Process)\n",
    "def refit_wilmink(cow_data):\n",
    "    x_data = cow_data['DaysInMilk'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Use initial guesses and bounds from the original fitting process\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "    popt, _ = curve_fit(wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=30000)\n",
    "    \n",
    "    # Calculate the expected yield with the refitted parameters\n",
    "    cow_data['ExpectedYield'] = wilmink_lactation_curve(cow_data['DaysInMilk'], *popt)\n",
    "    \n",
    "    # Calculate new residuals\n",
    "    cow_data['Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Define the Robust Wilmink Lactation Curve function\n",
    "def robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3):\n",
    "    dim = np.array(dim, dtype=np.float64)\n",
    "    days_in_milk = dim[0]\n",
    "    lag_1 = dim[1]\n",
    "    lag_2 = dim[2]\n",
    "    lag_3 = dim[3]\n",
    "    \n",
    "    return a + b * days_in_milk + c * np.exp(-d * days_in_milk) + lag1 * lag_1 + lag2 * lag_2 + lag3 * lag_3\n",
    "\n",
    "def fit_robust_wilmink(cow_data, lags=3):\n",
    "    cow_data = add_lagged_variables(cow_data, max_lag=lags)\n",
    "\n",
    "    # Extract individual columns from cow_data as separate arrays\n",
    "    days_in_milk = cow_data['DaysInMilk'].values\n",
    "    lag_1 = cow_data['lag_1'].values\n",
    "    lag_2 = cow_data['lag_2'].values\n",
    "    lag_3 = cow_data['lag_3'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Ensure all arrays have the same shape\n",
    "    assert len(days_in_milk) == len(lag_1) == len(lag_2) == len(lag_3) == len(y_data), \"Mismatch in data lengths\"\n",
    "\n",
    "    # Prepare initial guesses and bounds\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1, 0, 0, 0]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0, -np.inf, -np.inf, -np.inf], \n",
    "              [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "    \n",
    "    try:\n",
    "        # Pass individual components of x_data to curve_fit\n",
    "        popt, _ = curve_fit(\n",
    "            lambda dim, a, b, c, d, lag1, lag2, lag3: robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3), \n",
    "            (days_in_milk, lag_1, lag_2, lag_3), \n",
    "            y_data, \n",
    "            p0=initial_guesses, \n",
    "            bounds=bounds, \n",
    "            maxfev=50000\n",
    "        )\n",
    "\n",
    "        cow_data.loc[:, 'ExpectedYield'] = robust_wilmink_lactation_curve(\n",
    "            (days_in_milk, lag_1, lag_2, lag_3), *popt\n",
    "        )\n",
    "        cow_data.loc[:, 'Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Curve fitting failed: {e}\")\n",
    "        cow_data['ExpectedYield'] = np.nan\n",
    "        cow_data['Residuals'] = np.nan\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    \n",
    "    # Check for missing values and drop rows with NaNs in the lagged columns or DailyYield\n",
    "    cow_data_cleaned = cow_data.dropna(subset=['DailyYield'] + [f'lag_{lag}' for lag in range(1, max_lag + 1)])\n",
    "    \n",
    "    # Ensure we're not dropping too much data, and there's still sufficient data for fitting\n",
    "    if len(cow_data_cleaned) == 0:\n",
    "        raise ValueError(\"Insufficient data after adding lagged variables. Check for missing data.\")\n",
    "\n",
    "    return cow_data_cleaned\n",
    "\n",
    "\n",
    "# Apply lagged variables to all cases, all lactations\n",
    "for se_number in data_cleaned['SE_Number'].unique():\n",
    "    for lactation_number in data_cleaned[data_cleaned['SE_Number'] == se_number]['LactationNumber'].unique():\n",
    "        \n",
    "        cow_data = data_cleaned[(data_cleaned['SE_Number'] == se_number) & \n",
    "                                (data_cleaned['LactationNumber'] == lactation_number)].copy()\n",
    "        \n",
    "        # Apply lagged variables for all cases, regardless of autocorrelation\n",
    "        cow_data = add_lagged_variables(cow_data, max_lag=3)\n",
    "        cow_data_refitted = fit_robust_wilmink(cow_data, lags=3)\n",
    "        \n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "\n",
    "# Remove rows where ExpectedYield is NaN\n",
    "data_cleaned = data_cleaned.dropna(subset=['ExpectedYield']).reset_index(drop=True)\n",
    "\n",
    "# Normalize yields\n",
    "data_cleaned['NormalizedDailyYield'] = data_cleaned['DailyYield'] / data_cleaned['ExpectedYield']\n",
    "data_cleaned['NormalizedDailyYieldChange'] = data_cleaned['DailyYieldChange'] / data_cleaned['ExpectedYield']\n",
    "\n",
    "\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OLD CODE\n",
    "# Define the Wilmink Lactation Curve function\n",
    "def wilmink_lactation_curve(dim, a, b, c, d):\n",
    "    dim = np.array(dim, dtype=float)\n",
    "    return a + b * dim + c * np.exp(-d * dim)\n",
    "\n",
    "# Function to directly refit the Wilmink Lactation Curve (Standard Process)\n",
    "def refit_wilmink(cow_data):\n",
    "    x_data = cow_data['DaysInMilk'].values\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "\n",
    "    # Use initial guesses and bounds from the original fitting process\n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "    popt, _ = curve_fit(wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=30000)\n",
    "    \n",
    "    # Calculate the expected yield with the refitted parameters\n",
    "    cow_data['ExpectedYield'] = wilmink_lactation_curve(cow_data['DaysInMilk'], *popt)\n",
    "    \n",
    "    # Calculate new residuals\n",
    "    cow_data['Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Define the Robust Wilmink Lactation Curve function\n",
    "def robust_wilmink_lactation_curve(dim, a, b, c, d, lag1, lag2, lag3):\n",
    "    dim = np.array(dim, dtype=np.float64)\n",
    "    days_in_milk = dim[0]\n",
    "    lag_1 = dim[1]\n",
    "    lag_2 = dim[2]\n",
    "    lag_3 = dim[3]\n",
    "    \n",
    "    return a + b * days_in_milk + c * np.exp(-d * days_in_milk) + lag1 * lag_1 + lag2 * lag_2 + lag3 * lag_3\n",
    "\n",
    "# Function to fit the robust Wilmink model\n",
    "def fit_robust_wilmink(cow_data, lags=3):\n",
    "    cow_data = add_lagged_variables(cow_data, max_lag=lags)\n",
    "    \n",
    "    x_data = cow_data[['DaysInMilk', 'lag_1', 'lag_2', 'lag_3']].values.T\n",
    "    y_data = cow_data['DailyYield'].values\n",
    "    \n",
    "    initial_guesses = [np.mean(y_data), 0, np.mean(y_data) / 2, 0.1, 0, 0, 0]\n",
    "    bounds = ([-np.inf, -np.inf, -np.inf, 0, -np.inf, -np.inf, -np.inf], \n",
    "              [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(robust_wilmink_lactation_curve, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=50000)\n",
    "        cow_data.loc[:, 'ExpectedYield'] = robust_wilmink_lactation_curve(x_data, *popt)\n",
    "        cow_data.loc[:, 'Residuals'] = cow_data['DailyYield'] - cow_data['ExpectedYield']\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Curve fitting failed: {e}\")\n",
    "        cow_data.loc[:, 'ExpectedYield'] = np.nan\n",
    "        cow_data.loc[:, 'Residuals'] = np.nan\n",
    "    \n",
    "    return cow_data\n",
    "\n",
    "# Function to add lagged variables for addressing autocorrelation\n",
    "def add_lagged_variables(cow_data, max_lag=3):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        cow_data[f'lag_{lag}'] = cow_data['DailyYield'].shift(lag)\n",
    "    return cow_data.dropna()\n",
    "\n",
    "# Example usage: Applying the robust model to flagged cases\n",
    "for index, row in flagged_df.iterrows():\n",
    "    se_number = row['SE_Number']\n",
    "    lactation_number = row['LactationNumber']\n",
    "    \n",
    "    cow_data = data_cleaned[(data_cleaned['SE_Number'] == se_number) & \n",
    "                            (data_cleaned['LactationNumber'] == lactation_number)].copy()\n",
    "    \n",
    "    if abs(row['ACF[1]']) > 0.2:  # Significant autocorrelation\n",
    "        cow_data = add_lagged_variables(cow_data, max_lag=3)\n",
    "        cow_data_refitted = fit_robust_wilmink(cow_data, lags=3)\n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "    else:\n",
    "        cow_data_refitted = refit_wilmink(cow_data)\n",
    "        data_cleaned.update(cow_data_refitted)\n",
    "\n",
    "# Erase all rows where ExpectedYield is NaN\n",
    "data_cleaned = data_cleaned.dropna(subset=['ExpectedYield']).reset_index(drop=True)\n",
    "\n",
    "data_cleaned['NormalizedDailyYield'] = data_cleaned['DailyYield'] / data_cleaned['ExpectedYield']\n",
    "data_cleaned['NormalizedDailyYieldChange'] = data_cleaned['DailyYieldChange'] / data_cleaned['ExpectedYield']\n",
    "\n",
    "data_cleaned\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "mean_residual_threshold = 0.075\n",
    "std_residual_threshold = 7.5\n",
    "acf_threshold = 0.25\n",
    "pacf_threshold = 0.25\n",
    "\n",
    "# List to collect flagged combinations\n",
    "flagged_combinations = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals'].dropna()\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Check against thresholds\n",
    "            if (abs(acf_values[1]) > acf_threshold or \n",
    "                abs(pacf_values[1]) > pacf_threshold):\n",
    "                \n",
    "                # Collect the combination if it exceeds any threshold\n",
    "                flagged_combinations.append({\n",
    "                    'Farm': farm_name,\n",
    "                    'SE_Number': se_number,\n",
    "                    'LactationNumber': lactation_number,\n",
    "                    'ACF[1]': acf_values[1],\n",
    "                    'PACF[1]': pacf_values[1]\n",
    "                })\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "flagged_df = pd.DataFrame(flagged_combinations)\n",
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, threshold=3.5):\n",
    "    # Calculate z-scores of residuals\n",
    "    data = data.copy()  # Create a copy to avoid the SettingWithCopyWarning\n",
    "    data['z_score'] = (data['Residuals'] - data['Residuals'].mean()) / data['Residuals'].std()\n",
    "    \n",
    "    # Identify the number of outliers\n",
    "    num_outliers = (data['z_score'].abs() >= threshold).sum()\n",
    "    print(f\"Number of outliers detected: {num_outliers}\")\n",
    "    \n",
    "    # Remove rows where the z-score of the residual is greater than the threshold\n",
    "    cleaned_data = data.loc[(data['z_score'].abs() < threshold)].drop(columns=['z_score'])\n",
    "    \n",
    "    # Print the number of rows before and after\n",
    "    print(f\"Number of rows before outlier removal: {len(data)}\")\n",
    "    print(f\"Number of rows after outlier removal: {len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Apply to flagged cases\n",
    "for index, row in flagged_df.iterrows():\n",
    "    se_number = row['SE_Number']\n",
    "    lactation_number = row['LactationNumber']\n",
    "    \n",
    "    # Select the cow data for the specific SE_Number and LactationNumber\n",
    "    cow_data = data_cleaned.loc[(data_cleaned['SE_Number'] == se_number) & \n",
    "                                (data_cleaned['LactationNumber'] == lactation_number)]\n",
    "    \n",
    "    # Remove outliers\n",
    "    cow_data_trimmed = remove_outliers(cow_data, threshold=3.5)\n",
    "    \n",
    "    # Recalculate the residuals and update the dataset\n",
    "    cow_data_trimmed['Residuals'] = cow_data_trimmed['DailyYield'] - cow_data_trimmed['ExpectedYield']\n",
    "    \n",
    "    # Remove the old data for this cow from data_cleaned\n",
    "    data_cleaned = data_cleaned.loc[~((data_cleaned['SE_Number'] == se_number) & \n",
    "                                      (data_cleaned['LactationNumber'] == lactation_number))]\n",
    "    \n",
    "    # Append the cleaned data back to data_cleaned\n",
    "    data_cleaned = pd.concat([data_cleaned, cow_data_trimmed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "mean_residual_threshold = 0.075\n",
    "std_residual_threshold = 7.5\n",
    "acf_threshold = 0.25\n",
    "pacf_threshold = 0.25\n",
    "\n",
    "# List to collect flagged combinations\n",
    "flagged_combinations = []\n",
    "\n",
    "for farm_name, farm_group in data_cleaned.groupby('FarmName_Pseudo'):\n",
    "    for (se_number, lactation_number), cow_group in farm_group.groupby(['SE_Number', 'LactationNumber']):\n",
    "        residuals = cow_group['Residuals'].dropna()\n",
    "        \n",
    "        if len(residuals) > 1:  # Ensure there are residuals to analyze\n",
    "            acf_values = acf(residuals, nlags=30, fft=False)\n",
    "            pacf_values = pacf(residuals, nlags=min(30, len(residuals)//2))\n",
    "\n",
    "            # Check against thresholds\n",
    "            if (abs(acf_values[1]) > acf_threshold or \n",
    "                abs(pacf_values[1]) > pacf_threshold):\n",
    "                \n",
    "                # Collect the combination if it exceeds any threshold\n",
    "                flagged_combinations.append({\n",
    "                    'Farm': farm_name,\n",
    "                    'SE_Number': se_number,\n",
    "                    'LactationNumber': lactation_number,\n",
    "                    'ACF[1]': acf_values[1],\n",
    "                    'PACF[1]': pacf_values[1]\n",
    "                })\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "flagged_df = pd.DataFrame(flagged_combinations)\n",
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "new_order = [\n",
    "    \"Date\", \"FarmName_Pseudo\", \"SE_Number\", \"Age\", \"BreedName\", \"LactationNumber\", \"DaysInMilk\",'YearSeason', \"DailyYield\", \"PreviousDailyYield\", \n",
    "    \"DailyYieldChange\", \"ExpectedYield\", \"NormalizedDailyYield\", \n",
    "    \"NormalizedDailyYieldChange\", \"Residuals\", \"HeatStress\", \"Temp15Threshold\", \"HW\", \n",
    "    \"cum_HW\", \"MeanTemperature\", \"MeanTHI_adj\", \"HeatLoad\", \"CumulativeHeatLoad\"\n",
    "]\n",
    "data_cleaned = data_cleaned[new_order]\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NormalizedDailyYield is centered around 1 for each unique farm\n",
    "print(\"Mean of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].mean())\n",
    "print(\"Standard Deviation of NormalizedDailyYield:\", data_cleaned.groupby('FarmName_Pseudo')['NormalizedDailyYield'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of HeatStress occurrences in each farm\n",
    "heat_stress_counts = data_cleaned.groupby('FarmName_Pseudo')['HeatStress'].sum()\n",
    "heat_stress_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of observations within each farm\n",
    "no_obs = data_cleaned.groupby('FarmName_Pseudo').size().reset_index(name='count')\n",
    "no_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reordered DataFrame to a CSV file - 61 degrees threshold\n",
    "# data_cleaned.to_csv('../Data/MergedData/HeatApproachCleanedYieldDataTestQuantile61.csv', index=False)\n",
    "# print(data_cleaned.shape)\n",
    "\n",
    "# 67 degrees threshold\n",
    "data_cleaned.to_csv('../Data/MergedData/HeatApproachCleanedYieldDataTestQuantile67.csv', index=False)\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desk stat for article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No. of milk days in file after Quantile regression program: {data_cleaned.shape}\")\n",
    "\n",
    "data_cleaned2 = data_cleaned.drop_duplicates(subset=[\"SE_Number\", \"LactationNumber\"])\n",
    "print(f\"No. of lactations in file after Quantile regression program: {data_cleaned2.shape}\")\n",
    "\n",
    "data_cleaned2 = data_cleaned.drop_duplicates(subset=[\"SE_Number\"])\n",
    "print(f\"No. of lactations in file after Quantile regression program: {data_cleaned2.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
